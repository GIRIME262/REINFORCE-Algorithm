{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Muestrar {$\\tau^i$} de $\\pi_{\\theta}(a_t|s_t)$ - Correr M trayectorias usando la policy\n",
    "### 2. Estimar el retorno: $$ R(\\tau_i)  \\approx \\sum_{t=0}^{T}R(s_t^i, a_t^i)$$\n",
    "### 3. Entrenar un modelo: $$ \\nabla_{\\theta} J_{\\theta} \\approx \\frac{1}{M} \\sum_{i=1}^{M}  R(\\tau_i)   \\sum_{t=0}^T \\nabla_{\\theta} log \\pi_{\\theta}(a_t^i|s_t^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suponiendo que solo corremos un episodio por iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La loss queda (Le ponemos un menos adelante para que tengamos que minimizar):\n",
    "\n",
    "$$ \\huge J_{\\theta} =  - R \\sum_{t=0}^T log \\pi_{\\theta}(a_t|s_t)$$\n",
    "$$ \\huge J_{\\theta} =  - \\sum_{t=0}^T log \\pi_{\\theta}(a_t|s_t) R $$\n",
    "$$ \\huge J_{\\theta} =  \\sum_{t=0}^T log \\frac{1}{\\pi_{\\theta}(a_t|s_t)} R $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando la Entropía cruzada:\n",
    "\n",
    "$$ \\huge H(y_{true}, y_{pred}) = \\sum_{i} y_{true_i} log (\\frac{1}{y_{pred_i}}) $$\n",
    "\n",
    "Ejemplo: \n",
    "\n",
    "- Sumpongamos que tenemos 3 acciones posibles y la red neuronal predijo $y_{pred}$ = [0.2, 0.3, 0.5]\n",
    "- Se muestreó la salida y se eligión la acción 2, es decir la acción con probabilidad 0.3\n",
    "- La $y_{true}$ será [0, 1, 0]\n",
    "\n",
    "$$ \\huge H = 0 log (\\frac{1}{0.2}) + 1 log (\\frac{1}{0.3}) + 0 log (\\frac{1}{0.5}) $$\n",
    "\n",
    "- Si redefinimos la $y_{true}$ como $y_{true}$ = $y_{true} R$\n",
    "- La $y_{true}$ queda [0, R, 0]\n",
    "\n",
    "$$ \\huge H = 0 log (\\frac{1}{0.2}) + R log (\\frac{1}{0.3}) + 0 log (\\frac{1}{0.5}) = R log (\\frac{1}{\\pi_{\\theta}(a_t|s_t)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge tensorboardx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE_helper import BaseAgent\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent(BaseAgent):\n",
    "    # def __init__(self):\n",
    "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
    "        ## Defino métrica - loss sin el retorno multiplicando\n",
    "        def loss_metric(y_true, y_pred):\n",
    "            y_true_norm = K.sign(y_true)\n",
    "            return K.categorical_crossentropy(y_true_norm, y_pred)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='relu'))\n",
    "        model.add(Dense(output_shape, activation='softmax'))\n",
    "        ## Por que la categorical_crossentropy funciona ok?\n",
    "        model.compile(Adam(lr), loss=['categorical_crossentropy'], metrics=[loss_metric])\n",
    "        return model\n",
    "    \n",
    "    def get_action(self, eval=False):\n",
    "        p = self.model.predict([self.observation.reshape(1, self.nS)])\n",
    "        if eval is False:\n",
    "            action = np.random.choice(self.nA, p=p[0]) #np.nan_to_num(p[0])\n",
    "        else:\n",
    "            action = np.argmax(p[0])\n",
    "        action_one_hot = np.zeros(self.nA)\n",
    "        action_one_hot[action] = 1\n",
    "        return action, action_one_hot, p\n",
    "    \n",
    "    def get_entropy(self, preds, epsilon=1e-12):\n",
    "        entropy = np.mean(-np.sum(np.log(preds+epsilon)*preds, axis=1)/np.log(self.nA))\n",
    "        return entropy\n",
    "    \n",
    "    def get_discounted_rewards(self, r):\n",
    "        # Por si es una lista\n",
    "        r = np.array(r, dtype=float)\n",
    "        \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, r.size)):\n",
    "            running_add = running_add * self.gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Ejemplos para probar get_discounted_rewards:\n",
    "\n",
    "reinforce_agent.get_discounted_rewards([1, 1, 1, 1, 1, 1])\n",
    "array([5.98501999, 4.99001   , 3.994004  , 2.997001  , 1.999     ,\n",
    "       1.        ])\n",
    "\n",
    "reinforce_agent.get_discounted_rewards([1, 2, 3, 4, 5, 6])\n",
    "array([20.93010492, 19.95005497, 17.96802299, 14.983006  , 10.994     ,\n",
    "        6.        ])\n",
    "\n",
    "reinforce_agent.get_discounted_rewards([5, 4, -3, -4, 5, 8])\n",
    "array([14.9540949 ,  9.96405896,  5.97002899,  8.979008  , 12.992     ,\n",
    "        8.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/julianganzabal/anaconda3/envs/mllab/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'logs/CartPole-v1/REINFORCE/1_1_0.999_0.001_1574895170'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent = ReinforceAgent('CartPole-v1', n_experience_episodes=1)\n",
    "reinforce_agent.logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reinforce_agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0\n",
      "action_one_hot: [1. 0.]\n",
      "Policy prob dist: [[0.50359005 0.49640995]]\n"
     ]
    }
   ],
   "source": [
    "reinforce_agent.reset_env()\n",
    "action, action_one_hot, p = reinforce_agent.get_action()\n",
    "print('Action:', action)\n",
    "print('action_one_hot:', action_one_hot)\n",
    "print('Policy prob dist:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_agent.reset_env()\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, last_obs = reinforce_agent.get_experience_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observaciones:\n",
      "[[ 1.94014565e-04  1.86248154e-02 -2.69223726e-02  1.34183187e-02]\n",
      " [ 5.66510874e-04 -1.76100896e-01 -2.66540062e-02  2.97486816e-01]\n",
      " [-2.95550705e-03  1.93906708e-02 -2.07042699e-02 -3.48172724e-03]\n",
      " [-2.56769363e-03  2.14803338e-01 -2.07739044e-02 -3.02624526e-01]\n",
      " [ 1.72837313e-03  4.10215106e-01 -2.68263949e-02 -6.01786020e-01]\n",
      " [ 9.93267525e-03  2.15478475e-01 -3.88621153e-02 -3.17672189e-01]\n",
      " [ 1.42422447e-02  2.09309653e-02 -4.52155591e-02 -3.74938466e-02]\n",
      " [ 1.46608641e-02 -1.73514406e-01 -4.59654360e-02  2.40587266e-01]\n",
      " [ 1.11905759e-02  2.22329932e-02 -4.11536907e-02 -6.62327830e-02]\n",
      " [ 1.16352358e-02 -1.72275505e-01 -4.24783464e-02  2.13187161e-01]\n",
      " [ 8.18972569e-03  2.34272088e-02 -3.82146032e-02 -9.25869824e-02]\n",
      " [ 8.65826986e-03 -1.71126763e-01 -4.00663428e-02  1.87798554e-01]\n",
      " [ 5.23573460e-03  2.45448336e-02 -3.63103717e-02 -1.17249641e-01]\n",
      " [ 5.72663127e-03 -1.70038555e-01 -3.86553645e-02  1.63760149e-01]\n",
      " [ 2.32586018e-03 -3.64586435e-01 -3.53801616e-02  4.44002123e-01]\n",
      " [-4.96586851e-03 -1.68982215e-01 -2.65001191e-02  1.40379813e-01]\n",
      " [-8.34551281e-03  2.65090566e-02 -2.36925228e-02 -1.60544259e-01]\n",
      " [-7.81533168e-03 -1.68265833e-01 -2.69034080e-02  1.24571129e-01]\n",
      " [-1.11806483e-02  2.72310075e-02 -2.44119854e-02 -1.76476662e-01]\n",
      " [-1.06360282e-02 -1.67533230e-01 -2.79415187e-02  1.08406236e-01]\n",
      " [-1.39866928e-02 -3.62243867e-01 -2.57733940e-02  3.92144450e-01]\n",
      " [-2.12315701e-02 -5.56990757e-01 -1.79305050e-02  6.76591227e-01]\n",
      " [-3.23713853e-02 -3.61624314e-01 -4.39868042e-03  3.78317384e-01]\n",
      " [-3.96038715e-02 -5.56683520e-01  3.16766725e-03  6.69610155e-01]\n",
      " [-5.07375419e-02 -7.51849371e-01  1.65598704e-02  9.63288755e-01]\n",
      " [-6.57745294e-02 -5.56953789e-01  3.58256455e-02  6.75853845e-01]\n",
      " [-7.69136052e-02 -3.62347497e-01  4.93427224e-02  3.94662203e-01]\n",
      " [-8.41605551e-02 -1.67959151e-01  5.72359664e-02  1.17935446e-01]\n",
      " [-8.75197381e-02  2.62980067e-02  5.95946753e-02 -1.56154950e-01]\n",
      " [-8.69937780e-02  2.20518310e-01  5.64715763e-02 -4.29457759e-01]\n",
      " [-8.25834118e-02  2.46440082e-02  4.78824212e-02 -1.19520849e-01]\n",
      " [-8.20905316e-02  2.19048383e-01  4.54920042e-02 -3.96721136e-01]\n",
      " [-7.77095639e-02  2.33115123e-02  3.75575815e-02 -9.00495517e-02]\n",
      " [-7.72433337e-02 -1.72328083e-01  3.57565904e-02  2.14242226e-01]\n",
      " [-8.06898954e-02 -3.67942506e-01  4.00414349e-02  5.17986504e-01]\n",
      " [-8.80487455e-02 -5.63604676e-01  5.04011650e-02  8.23013545e-01]\n",
      " [-9.93208390e-02 -3.69207138e-01  6.68614359e-02  5.46599197e-01]\n",
      " [-1.06704982e-01 -5.65201635e-01  7.77934199e-02  8.59576668e-01]\n",
      " [-1.18009014e-01 -3.71220563e-01  9.49849532e-02  5.92333274e-01]\n",
      " [-1.25433426e-01 -5.67534885e-01  1.06831619e-01  9.13360978e-01]\n",
      " [-1.36784123e-01 -7.63927161e-01  1.25098838e-01  1.23761871e+00]\n",
      " [-1.52062667e-01 -5.70614408e-01  1.49851212e-01  9.86598791e-01]\n",
      " [-1.63474955e-01 -3.77782215e-01  1.69583188e-01  7.44483579e-01]\n",
      " [-1.71030599e-01 -5.74788020e-01  1.84472860e-01  1.08537210e+00]\n",
      " [-1.82526360e-01 -7.71800130e-01  2.06180302e-01  1.42980417e+00]]\n",
      "Acciones:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Policy prob dist:\n",
      "[[0.5008111  0.49918896]\n",
      " [0.4950956  0.5049043 ]\n",
      " [0.5005936  0.49940643]\n",
      " [0.49351048 0.5064896 ]\n",
      " [0.48732772 0.51267225]\n",
      " [0.49482957 0.50517046]\n",
      " [0.50128466 0.49871534]\n",
      " [0.49558726 0.5044127 ]\n",
      " [0.5010243  0.49897566]\n",
      " [0.4962544  0.5037456 ]\n",
      " [0.50128764 0.49871227]\n",
      " [0.4971156  0.50288445]\n",
      " [0.5015375  0.49846253]\n",
      " [0.49813712 0.5018629 ]\n",
      " [0.4902939  0.50970614]\n",
      " [0.49894556 0.5010544 ]\n",
      " [0.5022616  0.49773848]\n",
      " [0.49981922 0.50018084]\n",
      " [0.50266165 0.4973383 ]\n",
      " [0.50069845 0.4993015 ]\n",
      " [0.4921414  0.5078586 ]\n",
      " [0.48508406 0.51491594]\n",
      " [0.4923354  0.50766456]\n",
      " [0.48552224 0.5144777 ]\n",
      " [0.47870365 0.5212963 ]\n",
      " [0.48578194 0.5142181 ]\n",
      " [0.4923093  0.5076907 ]\n",
      " [0.5005686  0.49943143]\n",
      " [0.5033248  0.4966752 ]\n",
      " [0.4937927  0.5062073 ]\n",
      " [0.5029241  0.49707595]\n",
      " [0.49331257 0.50668746]\n",
      " [0.50286174 0.49713826]\n",
      " [0.49838614 0.5016139 ]\n",
      " [0.49215916 0.5078408 ]\n",
      " [0.48528928 0.5147107 ]\n",
      " [0.49194482 0.5080552 ]\n",
      " [0.4849746  0.51502544]\n",
      " [0.49035093 0.50964904]\n",
      " [0.48408425 0.51591575]\n",
      " [0.47762236 0.52237767]\n",
      " [0.48165932 0.51834065]\n",
      " [0.48517445 0.5148256 ]\n",
      " [0.47887155 0.5211285 ]\n",
      " [0.4718082  0.5281918 ]]\n",
      "Discounted Sum of Rewards:\n",
      "[44.02404222 43.06710933 42.10921855 41.15036892 40.19055947 39.22978926\n",
      " 38.26805732 37.30536268 36.34170439 35.37708147 34.41149296 33.4449379\n",
      " 32.47741532 31.50892424 30.5394637  29.56903274 28.59763037 27.62525562\n",
      " 26.65190753 25.67758512 24.7022874  23.72601342 22.74876218 21.77053271\n",
      " 20.79132404 19.81113517 18.82996514 17.84781295 16.86467763 15.88055818\n",
      " 14.89545364 13.909363   12.92228529 11.93421951 10.94516467  9.95511979\n",
      "  8.96408387  7.97205593  6.97903497  5.98501999  4.99001     3.994004\n",
      "  2.997001    1.999       1.        ]\n",
      "Return copiado para cada acción:\n",
      "[44.02404222 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222\n",
      " 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222\n",
      " 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222\n",
      " 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222\n",
      " 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222\n",
      " 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222\n",
      " 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222 44.02404222\n",
      " 44.02404222 44.02404222 44.02404222]\n",
      "Longitud del episodia: [45]\n",
      "Ultima observación: [-0.19796236 -0.57973271  0.23477639  1.20799346]\n"
     ]
    }
   ],
   "source": [
    "print('Observaciones:'), print(obs), print('Acciones:'), print(actions)\n",
    "print('Policy prob dist:'), print(preds)\n",
    "print('Discounted Sum of Rewards:')\n",
    "print(disc_sum_rews)\n",
    "print('Return copiado para cada acción:')\n",
    "print(ep_returns)\n",
    "print('Longitud del episodia:', ep_len)\n",
    "print('Ultima observación:', last_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996788"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.get_entropy(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE_helper import RunningVariance\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correr en linea de comando: tensorboard --logdir logs/\n",
      "Episode: 951"
     ]
    }
   ],
   "source": [
    "reinforce_agent = ReinforceAgent('CartPole-v1', n_experience_episodes=50, EPISODES=2000, epochs=20, lr=0.001)\n",
    "running_variance = RunningVariance()\n",
    "initial_time = time()\n",
    "\n",
    "\n",
    "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
    "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, last_obs = reinforce_agent.get_experience_episodes()\n",
    "    for dr in ep_returns:\n",
    "        running_variance.add(dr)\n",
    "        \n",
    "    pseudolabels = actions*ep_returns.reshape(-1, 1)\n",
    "\n",
    "    history = reinforce_agent.model.fit(obs, pseudolabels, verbose=0, epochs=reinforce_agent.epochs, batch_size=128)\n",
    "    \n",
    "    reinforce_agent.log_data(reinforce_agent.episode, \n",
    "                      history.history['loss'][0], \n",
    "                      np.mean(ep_len), \n",
    "                      reinforce_agent.get_entropy(preds), \n",
    "                      running_variance.get_variance(), \n",
    "                      history.history['loss_metric'][0], \n",
    "                      time() - initial_time, ep_returns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
